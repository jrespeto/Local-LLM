
services:
  langflow:
    hostname: langflow
    profiles:
      - langflow
      - all
    image: docker.io/langflowai/langflow:latest
    pull_policy: always
    restart: unless-stopped
    ports:
      - "127.0.0.1:7860:7860"
    depends_on:
      - postgres
    environment:
      - LANGFLOW_DATABASE_URL=postgresql://langflow:langflow@postgres:5432/langflow
      # This variable defines where the logs, file storage, monitor data and secret keys are stored.
      - LANGFLOW_CONFIG_DIR=/app/langflow
      - DO_NOT_TRACK=true
    volumes:
      - langflow-data:/app/langflow

  postgres:
    image: docker.io/postgres:16
    restart: unless-stopped
    hostname: postgres
    profiles:
      - langflow
      - all
    environment:
      POSTGRES_USER: langflow
      POSTGRES_PASSWORD: langflow
      POSTGRES_DB: langflow
    # ports:
    #   - "127.0.0.1:5432:5432"
    volumes:
      - langflow-postgres:/var/lib/postgresql/data

  qdrant:
    image: docker.io/qdrant/qdrant
    restart: unless-stopped
    hostname: qdrant
    profiles:
      - langflow
      - openwebui
      - all
    # environment:
    #   QDRANT__SERVICE__API_KEY: 69745b10-e341-11ef-9f72-37374b5983cd
    # ports so we can use the web ui
    ports:
      - "127.0.0.1:6333:6333"
      - "127.0.0.1:6334:6334"
    volumes:
      - qdrant:/qdrant/storage

  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    restart: unless-stopped
    pull_policy: always
    hostname: openwebui
    profiles:
      - langflow
      - openwebui
      - all
    ports:
      - "127.0.0.1:8080:8080"
    volumes:
      - open-webui:/app/backend/data
    environment:
      ENV: dev
      ENABLE_WEBSOCKET_SUPPORT: true
      USE_CUDA_DOCKER: true
      WEBSOCKET_REDIS_URL: redis://redis:6379/1
      VECTOR_DB: qdrant
      QDRANT_URI: http://qdrant:6333
      OLLAMA_BASE_URL: http://ollama:11434
      COMFYUI_BASE_URL: http://comfyui:8188

  # Maybe one day this will work...
  # for now we need to start this from the command line
  # podman run -d --gpus=all --replace -v ~/ollama/.ollama:/root/.ollama -p 127.0.0.1:11434:11434 --name ollama docker.io/ollama/ollama
  # Run this with podman-compose v1.2 not podman compose
  ollama:
    hostname: ollama
    profiles:
      - langflow
      - ollama
      - openwebui
      - comfyui
      - all
    image: docker.io/ollama/ollama:latest
    pull_policy: always
    # ports:
    #   - "127.0.0.1:11434:11434"
    devices:
      - nvidia.com/gpu=all
    volumes:
    - ollama:/root/.ollama
    environment:
      - OLLAMA_NUM_PARALLEL=10
      - OLLAMA_MAX_QUEUE=10

  comfyui:
    hostname: comfyui
    profiles:
      - comfyui
      - all
    build:
      context: .
      dockerfile: docker/dockerfile.comfyui
    restart: on-failure
    ports:
      - "127.0.0.1:8188:8188"
    devices:
      - nvidia.com/gpu=all
    volumes:
    - comfyui:/comfyui

  redis:
    # container_name: redis
    image: docker.io/valkey/valkey:8-alpine
    command: valkey-server --save 30 1 --loglevel warning
    restart: on-failure
    hostname: redis
    profiles:
      - searxng
      - openwebui
      - all
    volumes:
      - redis-data:/data
    cap_drop:
      - ALL
    cap_add:
      - SETGID
      - SETUID
      - DAC_OVERRIDE
    logging:
      driver: "json-file"
      options:
        max-size: "1m"
        max-file: "1"

  searxng:
    # container_name: searxng
    # When setting up openwebui - use http://searxng:8080
    image: docker.io/searxng/searxng:latest
    restart: unless-stopped
    hostname: searxng
    profiles:
      - searxng
      - openwebui
      - all
    ports:
      - "127.0.0.1:8082:8080"
    volumes:
      - ./searxng:/etc/searxng:rw
    environment:
      - SEARXNG_BASE_URL=https://${SEARXNG_HOSTNAME:-localhost}/
      - UWSGI_WORKERS=${SEARXNG_UWSGI_WORKERS:-4}
      - UWSGI_THREADS=${SEARXNG_UWSGI_THREADS:-4}
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
    logging:
      driver: "json-file"
      options:
        max-size: "1m"
        max-file: "1"

  facefusion:
    hostname: facefusion
    profiles:
      - facefusion
      - all
    build:
      context: .
      dockerfile: docker/dockerfile.facefusion
    command: [ 'python', 'facefusion.py', 'run', '--execution-providers', 'cuda' ]
    volumes:
    - facefusion:/facefusion
    ports:
    - "127.0.0.1:7870:7860"
    devices:
      - nvidia.com/gpu=all

  nvidia:
    hostname: nvidia
    profiles:
      - test
    image: docker.io/nvidia/cuda:12.8.0-runtime-ubuntu24.04
    pull_policy: always
    command: nvidia-smi
    devices:
      - nvidia.com/gpu=all

volumes:
  langflow-postgres:
  langflow-data:
  qdrant:
  open-webui:
  redis-data:
  ollama:
  comfyui:
  facefusion:
